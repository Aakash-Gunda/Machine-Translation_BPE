{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "Machine_translation_bpe.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "pzML3GtyJMAg",
        "ZW5Ag2vZmFZb",
        "7ymRvBXYmhD_",
        "AfkzrvVcmq_C",
        "nuhE_dF6mv73"
      ]
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_D6_J-Uxlvb-"
      },
      "source": [
        "# Introduction to machine learning (ECE-GY 6143)\n",
        "## Machine Translation using byte pair encoding\n",
        "\n",
        "### Introduction\n",
        "In this notebook, we will see how to create a language translation model which is also a very famous application of neural machine translation. We will be using the packages from keras at first and we will also test the same model and dataset with Byte-pair encoding for better results. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzML3GtyJMAg"
      },
      "source": [
        "## Loading the Data\n",
        "\n",
        "Out first objective for this is to load the libraries and get the required data. For this project we have a french text file and an english text document each having 137860 sentences. In language translation projects this can be considered as a moderately sized data as we find models with very large data. We are currently using GPU provided by Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qeq3uQp1l1Ef"
      },
      "source": [
        "import re, collections\n",
        "\n",
        "import helper\n",
        "import numpy as np\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional, Dropout, LSTM\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.losses import sparse_categorical_crossentropy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.regularizers import L1L2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kb8Uk14Nl8OB"
      },
      "source": [
        "### Enabling and testing the GPU\n",
        "\n",
        "First, you'll need to enable GPUs for the notebook:\n",
        "\n",
        "- Navigate to Edit→Notebook Settings\n",
        "- select GPU from the Hardware Accelerator drop-down\n",
        "\n",
        "Next, we'll confirm that we can connect to the GPU with tensorflow:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSznnGr7l-X7",
        "outputId": "f81ac1f2-2741-4e76-85ca-fc451d14faf0"
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 17716559547697718621\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 11154422528\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 9717515621888176888\n",
            "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3B7T6c8LmAhZ"
      },
      "source": [
        "### Load Data\n",
        "\n",
        "We first load the data from our text files into english_sentences and french_sentences sentences respectively by reading each line of the text file by cleaning the line. We have found a preprocessed clean dataset, we just need to do small changes to it like removing the unnecessary trailing spaces, converting the text to lower case,and removing punctuation to create better bag of vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vH7BBWgfmDXD",
        "outputId": "666796cf-daa9-4d3b-f19d-06dbc2f40f2d"
      },
      "source": [
        "# Load English data\n",
        "english_sentences = []\n",
        "max_len = 0\n",
        "with open('small_vocab_en.txt') as f:\n",
        "    for line in f:\n",
        "      if len(line) > max_len:\n",
        "        max_len = len(line)\n",
        "      line = line.rstrip()\n",
        "      line = line.lower()\n",
        "      line = re.sub(r'[^\\w\\s]', '', line)\n",
        "      english_sentences.append(line)\n",
        "# Load French data\n",
        "french_sentences = []\n",
        "with open('small_vocab_fr.txt') as f:\n",
        "    for line in f:\n",
        "      if len(line) > max_len:\n",
        "        max_len = len(line)\n",
        "      line = line.rstrip()\n",
        "      line = line.lower()\n",
        "      line = re.sub(r'[^\\w\\s]', '', line)\n",
        "      french_sentences.append(line)\n",
        "print('Dataset Loaded')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset Loaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AA1_jMzqMar3"
      },
      "source": [
        "### Sample english and french data in our dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAPFT0jcmPhR",
        "outputId": "7853dc2a-45a5-4370-cba1-6a7d195e8b33"
      },
      "source": [
        "for sample_i in range(2):\n",
        "    print('small_vocab_en Line {}:  {}'.format(sample_i + 1, english_sentences[sample_i]))\n",
        "    print('small_vocab_fr Line {}:  {}'.format(sample_i + 1, french_sentences[sample_i]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "small_vocab_en Line 1:  new jersey is sometimes quiet during autumn  and it is snowy in april \n",
            "small_vocab_fr Line 1:  new jersey est parfois calme pendant l automne  et il est neigeux en avril \n",
            "small_vocab_en Line 2:  the united states is usually chilly during july  and it is usually freezing in november \n",
            "small_vocab_fr Line 2:  les étatsunis est généralement froid en juillet  et il gèle habituellement en novembre \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZW5Ag2vZmFZb"
      },
      "source": [
        "## Tokenize From Keras\n",
        "\n",
        "Tokens are the building blocks of Natural Language. In order to get our computer to understand any text, we need to break that word down in a way that our machine can understand. Tokenization is a way of separating a piece of text into smaller units called tokens. Tokenization can be broadly classified into 3 types – word, character, and subword tokenization. \n",
        "\n",
        "Keras offers a library for word tokenization which can be used to create a bag of words which has all the words present in the dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iB7wKDeymdL9"
      },
      "source": [
        "def tokenize(x):\n",
        "\n",
        "    # Create the tokeninzer\n",
        "    t = Tokenizer()\n",
        "    # Create dictionary mapping words (str) to their rank/index (int)\n",
        "    t.fit_on_texts(x)\n",
        "    # Use the tokenizer to tokenize the text\n",
        "    text_sequences = t.texts_to_sequences(x)\n",
        "    return text_sequences, t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ymRvBXYmhD_"
      },
      "source": [
        "## Padding\n",
        "\n",
        "All the neural networks require to have inputs that have the same shape and size. But, we can't expect all the sentences to be of the same length. So, we use padding. Padding is the process of adding the zeros to the sequence to make all the samples in the same size.  \n",
        "\n",
        "We import pad_sequences from keras.preprocessing.sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ntkRXERmn8x"
      },
      "source": [
        "def pad(x, length=None):\n",
        "    \"\"\"\n",
        "    Pad x\n",
        "    :param x: List of sequences.\n",
        "    :param length: Length to pad the sequence to.  If None, use length of longest sequence in x.\n",
        "    :return: Padded numpy array of sequences\n",
        "    \"\"\"\n",
        "    # TODO: Implement\n",
        "    \n",
        "    # If length equals None, set it to be the length of the longest sequence in x\n",
        "    if length == None:\n",
        "        length = len(max(x, key=len))\n",
        "        \n",
        "    # Use Keras's pad_sequences to pad the sequences with 0's\n",
        "    padded_sequences = pad_sequences(sequences=x, maxlen=length, padding='post', value=0)\n",
        "    \n",
        "    return padded_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfkzrvVcmq_C"
      },
      "source": [
        "## Data Pre-processing of input data\n",
        "\n",
        "RNN models wont accept text as input they olnly take sequences of integers. So, we'll convert the text into sequences of integers using the following preprocess methods:\n",
        "\n",
        "1. Tokenize the words into ids\n",
        "2. Add padding to make all the sequences the same length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NI3V2bB1mYgC"
      },
      "source": [
        "def preprocess(x, y):\n",
        "    \"\"\"\n",
        "    Preprocess x and y\n",
        "    :param x: Feature List of sentences\n",
        "    :param y: Label List of sentences\n",
        "    :return: Tuple of (Preprocessed x, Preprocessed y, x tokenizer, y tokenizer)\n",
        "    \"\"\"\n",
        "    preprocess_x, x_tk = tokenize(x)\n",
        "    preprocess_y, y_tk = tokenize(y)\n",
        "\n",
        "    preprocess_x = pad(preprocess_x)\n",
        "    preprocess_y = pad(preprocess_y)\n",
        "\n",
        "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
        "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
        "\n",
        "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
        "    \n",
        "english_sentences_train, english_sentences_test, french_sentences_train, french_sentences_test =\\\n",
        "    train_test_split(english_sentences, french_sentences, test_size=0.2, random_state=42)\n",
        "\n",
        "preproc_english_sentences_train, preproc_french_sentences_train, english_tokenizer_train, french_tokenizer_train =\\\n",
        "    preprocess(english_sentences_train, french_sentences_train)\n",
        "    \n",
        "max_english_sequence_length_train = preproc_english_sentences_train.shape[1]\n",
        "max_french_sequence_length_train = preproc_french_sentences_train.shape[1]\n",
        "english_vocab_size_train = len(english_tokenizer_train.word_index)\n",
        "french_vocab_size_train = len(french_tokenizer_train.word_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHbwzqSnSrpb"
      },
      "source": [
        "### Tokeniztion of unknown sentences\n",
        "\n",
        "The below code displays how the sentence which is not used for tokenization i is tokenized into a sequence of integers. If you observe, some of the lists are empty. It is because of a problem called Out of Vocabulary (OOV). Out-of-vocabulary (OOV) are terms that are not part of the tokenized words found in a natural language processing environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REbUZbqonU1P",
        "outputId": "3d60249f-8ca0-4892-94c6-e2cca70c3403"
      },
      "source": [
        "Unknown_sentence = 'he dislikes grapefruit limes and lemons'\n",
        "Tok_data = tokenize(Unknown_sentence)\n",
        "print(Tok_data)\n",
        "## All the empty values represent Unknown data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "([[10], [1], [], [5], [2], [3], [4], [2], [11], [1], [3], [], [12], [6], [7], [13], [1], [14], [6], [15], [2], [16], [], [4], [2], [8], [1], [3], [], [7], [9], [5], [], [4], [1], [8], [17], [9], [3]], <keras_preprocessing.text.Tokenizer object at 0x7f095c3f8710>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuhE_dF6mv73"
      },
      "source": [
        "## Byte-Pair Encoding\n",
        "\n",
        "Byte-Pair Encoding aims to reduce the Out of Vocabulary words in a model with limited words in the dictionary by trying to combine word and character tokenization techniques. For example, if the model learned the relationship between \"old\", \"older\", and \"oldest\" then it could easily translate the word \"smarter\" or \"smartest\" if the dictionary has the word \"smart\" in it. A similar method can be used for a lot of words like words ending with \"ing\" (eat, eating), \"ed\" (learn, learned). We still can't tokenize for some words but it can tokenize more words than word-tokenization can which can be shown by using both the tokenizations on the same RNN model.\n",
        "\n",
        "The steps needed for BPE are\n",
        "\n",
        "1. Initialize vocabulary.\n",
        "2. Represent each word as a combination of the characters along with the special end of word token </w>.\n",
        "3. Iteratively count character pairs in all tokens of the vocabulary.\n",
        "4. Merge every occurrence of the most frequent pair, add the new character n-gram to the vocabulary.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrBTrj2fpshL"
      },
      "source": [
        "def tokenize_bpe(filename):\n",
        "  def get_vocab(filename):\n",
        "    # Creates a vocab from file\n",
        "    # Splits the word, removes punctuation, makes it lower case and adds </w> at the end\n",
        "    vocab = collections.defaultdict(int)\n",
        "    with open(filename) as f:\n",
        "        for line in f:\n",
        "            line = re.sub(r'[^\\w\\s]', '', line)\n",
        "            line = line.lower()\n",
        "            words = line.strip().split()\n",
        "            for word in words:\n",
        "                vocab[' '.join(list(word)) + ' </w>'] += 1\n",
        "\n",
        "    return vocab\n",
        "\n",
        "  def get_stats(vocab):\n",
        "      pairs = collections.defaultdict(int)\n",
        "      for word, freq in vocab.items():\n",
        "          symbols = word.split()\n",
        "          for i in range(len(symbols)-1):\n",
        "              pairs[symbols[i],symbols[i+1]] += freq\n",
        "      return pairs\n",
        "\n",
        "  def merge_vocab(pair, v_in):\n",
        "      v_out = {}\n",
        "      bigram = re.escape(' '.join(pair))\n",
        "      p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
        "      for word in v_in:\n",
        "          w_out = p.sub(''.join(pair), word)\n",
        "          v_out[w_out] = v_in[word]\n",
        "      return v_out\n",
        "\n",
        "  def get_tokens_from_vocab(vocab):\n",
        "      tokens_frequencies = collections.defaultdict(int)\n",
        "      vocab_tokenization = {}\n",
        "      for word, freq in vocab.items():\n",
        "          word_tokens = word.split()\n",
        "          for token in word_tokens:\n",
        "              tokens_frequencies[token] += freq\n",
        "          vocab_tokenization[''.join(word_tokens)] = word_tokens\n",
        "      return tokens_frequencies, vocab_tokenization\n",
        "\n",
        "  def measure_token_length(token):\n",
        "      if token[-4:] == '</w>':\n",
        "          return len(token[:-4]) + 1\n",
        "      else:\n",
        "          return len(token)\n",
        "\n",
        "  vocab = get_vocab(filename)\n",
        "  tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(vocab)\n",
        "\n",
        "  num_merges = 10000\n",
        "  for i in range(num_merges):\n",
        "      pairs = get_stats(vocab)\n",
        "      if not pairs:\n",
        "          break\n",
        "      best = max(pairs, key=pairs.get)\n",
        "      vocab = merge_vocab(best, vocab)\n",
        "      #print('Iter: {}'.format(i))\n",
        "      #print('Best pair: {}'.format(best))\n",
        "      tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(vocab)\n",
        "  return(tokens_frequencies, vocab_tokenization)\n",
        "def tokenize_word(string, sorted_tokens, unknown_token='</u>'):\n",
        "      \n",
        "    if string == '':\n",
        "        return []\n",
        "    if sorted_tokens == []:\n",
        "        return [unknown_token]\n",
        "\n",
        "    string_tokens = []\n",
        "    for i in range(len(sorted_tokens)):\n",
        "        token = sorted_tokens[i]\n",
        "        #token_reg = re.escape(token.replace('.', '[.]'))\n",
        "\n",
        "        matched_positions = [(m.start(0), m.end(0)) for m in re.finditer(token, string)]\n",
        "        if len(matched_positions) == 0:\n",
        "            continue\n",
        "        substring_end_positions = [matched_position[0] for matched_position in matched_positions]\n",
        "\n",
        "        substring_start_position = 0\n",
        "        for substring_end_position in substring_end_positions:\n",
        "            substring = string[substring_start_position:substring_end_position]\n",
        "            string_tokens += tokenize_word(string=substring, sorted_tokens=sorted_tokens[i+1:], unknown_token=unknown_token)\n",
        "            string_tokens += [token]\n",
        "            substring_start_position = substring_end_position + len(token)\n",
        "        remaining_substring = string[substring_start_position:]\n",
        "        string_tokens += tokenize_word(string=remaining_substring, sorted_tokens=sorted_tokens[i+1:], unknown_token=unknown_token)\n",
        "        break\n",
        "    return string_tokens\n",
        "def get_idx(x,sorted_tokens):\n",
        "  a = []\n",
        "  for i in x:\n",
        "    b = []\n",
        "    for j in i.split():\n",
        "      j = j+'</w>'\n",
        "      if j in sorted_tokens:\n",
        "        b.append(sorted_tokens[j])\n",
        "      else:\n",
        "        temp = tokenize_word(string=j, sorted_tokens=list(sorted_tokens.keys()), unknown_token='</u>')\n",
        "        if temp:\n",
        "          b.append(temp)\n",
        "        else:\n",
        "          b.append(0)\n",
        "    a.append(b)\n",
        "  return(a,list(sorted_tokens.keys()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6AY-ar9bga_"
      },
      "source": [
        "### Tokenizing input data\n",
        "\n",
        "Here we tokenize the input french and English sentences using Byte Pair Encoding. Here we are taking the number of merges as 10000, this is a hyper-parameter we set as per our data. As mentioned earlier, we append each word by '</w>'. This helps to recognize the end of words as the sentences are divided into letters before checking for pairs and '</w>' is used as the end of a word phrase. We also create a dictionary of words with indices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CLQyM-1pzPG"
      },
      "source": [
        "tokens_frequencies_en, vocab_tokenization_en = tokenize_bpe('small_vocab_en.txt')\n",
        "tokens_frequencies_fr, vocab_tokenization_fr = tokenize_bpe('small_vocab_fr.txt')\n",
        "k = 1\n",
        "for i in vocab_tokenization_en:\n",
        "  vocab_tokenization_en[i] = k\n",
        "  k += 1\n",
        "k = 1\n",
        "for i in vocab_tokenization_fr:\n",
        "  vocab_tokenization_fr[i] = k\n",
        "  k += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cs5VllFalDf"
      },
      "source": [
        "### Tokeniztion of unknown sentences\n",
        "\n",
        "The below code displays how the sentence which is not used for tokenization is tokenized into a sequence of integers using Byte pair encoding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APfdUUAkqXQi",
        "outputId": "00f71dbe-7d05-456f-c9f0-bc16fd62287e"
      },
      "source": [
        "text_sentences = [\n",
        "    'new jersey is sometimes quiet during autumn and it is snowy in april',\n",
        "    'he liked grapefruit limes and lemons']\n",
        "text_tokenized, text_tokenizer = get_idx(text_sentences,vocab_tokenization_en)\n",
        "print(text_tokenizer)\n",
        "print(text_tokenized)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['new</w>', 'jersey</w>', 'is</w>', 'sometimes</w>', 'quiet</w>', 'during</w>', 'autumn</w>', 'and</w>', 'it</w>', 'snowy</w>', 'in</w>', 'april</w>', 'the</w>', 'united</w>', 'states</w>', 'usually</w>', 'chilly</w>', 'july</w>', 'freezing</w>', 'november</w>', 'california</w>', 'march</w>', 'hot</w>', 'june</w>', 'mild</w>', 'cold</w>', 'september</w>', 'your</w>', 'least</w>', 'liked</w>', 'fruit</w>', 'grape</w>', 'but</w>', 'my</w>', 'apple</w>', 'his</w>', 'favorite</w>', 'orange</w>', 'paris</w>', 'relaxing</w>', 'december</w>', 'busy</w>', 'spring</w>', 'never</w>', 'our</w>', 'lemon</w>', 'january</w>', 'warm</w>', 'lime</w>', 'her</w>', 'banana</w>', 'he</w>', 'saw</w>', 'a</w>', 'old</w>', 'yellow</w>', 'truck</w>', 'india</w>', 'rainy</w>', 'that</w>', 'cat</w>', 'was</w>', 'most</w>', 'loved</w>', 'animal</w>', 'dislikes</w>', 'grapefruit</w>', 'limes</w>', 'lemons</w>', 'february</w>', 'china</w>', 'pleasant</w>', 'october</w>', 'wonderful</w>', 'nice</w>', 'summer</w>', 'france</w>', 'may</w>', 'grapes</w>', 'mangoes</w>', 'their</w>', 'mango</w>', 'pear</w>', 'august</w>', 'beautiful</w>', 'apples</w>', 'peaches</w>', 'feared</w>', 'shark</w>', 'wet</w>', 'dry</w>', 'we</w>', 'like</w>', 'oranges</w>', 'they</w>', 'pears</w>', 'she</w>', 'little</w>', 'red</w>', 'winter</w>', 'disliked</w>', 'rusty</w>', 'car</w>', 'strawberries</w>', 'i</w>', 'strawberry</w>', 'bananas</w>', 'going</w>', 'to</w>', 'next</w>', 'plan</w>', 'visit</w>', 'elephants</w>', 'were</w>', 'animals</w>', 'are</w>', 'likes</w>', 'dislike</w>', 'fall</w>', 'driving</w>', 'peach</w>', 'drives</w>', 'blue</w>', 'you</w>', 'bird</w>', 'horses</w>', 'mouse</w>', 'went</w>', 'last</w>', 'horse</w>', 'automobile</w>', 'dogs</w>', 'white</w>', 'elephant</w>', 'black</w>', 'think</w>', 'difficult</w>', 'translate</w>', 'between</w>', 'spanish</w>', 'portuguese</w>', 'big</w>', 'green</w>', 'translating</w>', 'fun</w>', 'where</w>', 'dog</w>', 'why</w>', 'might</w>', 'go</w>', 'this</w>', 'drove</w>', 'shiny</w>', 'sharks</w>', 'monkey</w>', 'how</w>', 'weather</w>', 'lion</w>', 'plans</w>', 'bear</w>', 'rabbit</w>', 'its</w>', 'chinese</w>', 'when</w>', 'eiffel</w>', 'tower</w>', 'did</w>', 'grocery</w>', 'store</w>', 'wanted</w>', 'does</w>', 'football</w>', 'field</w>', 'wants</w>', 'didnt</w>', 'snake</w>', 'snakes</w>', 'do</w>', 'easy</w>', 'thinks</w>', 'english</w>', 'french</w>', 'would</w>', 'arent</w>', 'cats</w>', 'rabbits</w>', 'has</w>', 'been</w>', 'monkeys</w>', 'lake</w>', 'bears</w>', 'school</w>', 'birds</w>', 'want</w>', 'isnt</w>', 'lions</w>', 'am</w>', 'mice</w>', 'have</w>']\n",
            "[[1, 2, 3, 4, 5, 6, 7, 8, 9, 3, 10, 11, 12], [52, 30, 67, 68, 8, 69]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGTV_6memLRy"
      },
      "source": [
        "### Data preprocessing using BPE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqQrge5ezzZL"
      },
      "source": [
        "def preprocess_BPE(english_sentences, french_sentences):\n",
        "  preprocess_x, x_tk = get_idx(english_sentences,vocab_tokenization_en)\n",
        "  preprocess_y, y_tk = get_idx(french_sentences,vocab_tokenization_fr)\n",
        "\n",
        "  preprocess_x = pad_sequences(preprocess_x,maxlen=max_len,padding='post')\n",
        "  preprocess_y = pad_sequences(preprocess_y,maxlen=max_len,padding='post')\n",
        "  preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
        "  return preprocess_x, preprocess_y, x_tk, y_tk\n",
        "\n",
        "preproc_english_sentences_bpe, preproc_french_sentences_bpe, english_tokenizer_bpe, french_tokenizer_bpe=preprocess_BPE(english_sentences_train, french_sentences_train)\n",
        "english_vocab_size_bpe = len(english_tokenizer_bpe)\n",
        "french_vocab_size_bpe = len(french_tokenizer_bpe)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6iO1Uyk0FqQ"
      },
      "source": [
        "## Models\n",
        "This sectionis to demonstrate word tokenization and byte pair encoding for two models\n",
        "\n",
        "- Model 1 is a simple RNN (Word tokenization, BPE)\n",
        "- Model 2 is a RNN with Embedding (BPE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcbJMhRiwm6K"
      },
      "source": [
        "### Ids Back to Text\n",
        "\n",
        "The neural network will be translating the input to a sequence of unreadable numbers, which isn't the final form we want. We want the translated french sentences.  The function 'logits_to_text' will convert the output integer sequences from the neural network to the French words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oD2vcazuz_nz",
        "outputId": "17ba2531-7455-4fcf-8b3a-c0c416c6dbd0"
      },
      "source": [
        "def logits_to_text(logits, tokenizer):\n",
        "\n",
        "\n",
        "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
        "    index_to_words[0] = '<PAD>'\n",
        "\n",
        "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
        "\n",
        "print('`logits_to_text` function loaded.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`logits_to_text` function loaded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8UhYSi1ptvT"
      },
      "source": [
        "### RNN Models\n",
        "\n",
        "A basic RNN model is a good baseline for sequence data.  In this model, you'll build a RNN that translates English to French.\n",
        "\n",
        "    Build and train a RNN model using word embedding on x and y\n",
        "    :param input_shape: Tuple of input shape\n",
        "    :param output_sequence_length: Length of output sequence\n",
        "    :param english_vocab_size: Number of unique English words in the dataset\n",
        "    :param french_vocab_size: Number of unique French words in the dataset\n",
        "    :return: Keras model built, but not trained\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2mxhe480Rz2",
        "outputId": "eded9dcd-d330-4d00-daa7-0d1307b71137"
      },
      "source": [
        "def embed_model_split(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "\n",
        "    # RNN model with embedding layer\n",
        "    input_layer = Input(shape=input_shape[1:])\n",
        "    embedding_layer = Embedding(512, english_vocab_size)(input_layer)\n",
        "    x = GRU(512, return_sequences=True)(embedding_layer)\n",
        "    x = TimeDistributed(Dense(french_vocab_size*4, activation='relu'))(x)\n",
        "    output_layer = Dense(french_vocab_size, activation='softmax')(x)\n",
        "    \n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "    \n",
        "    # Compile the model\n",
        "    learning_rate = 0.01\n",
        "    opt = Adam(learning_rate=learning_rate)\n",
        "    model.compile(loss=sparse_categorical_crossentropy,\n",
        "                  optimizer=opt,\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Pad the input \n",
        "tmp_x = pad(preproc_english_sentences_train, preproc_french_sentences_train.shape[1])\n",
        "\n",
        "# Train the neural network on the training data\n",
        "embedding_model_split_train = embed_model_split(tmp_x.shape,\n",
        "                                  max_french_sequence_length_train,\n",
        "                                  english_vocab_size_train + 1,\n",
        "                                  french_vocab_size_train + 1)\n",
        "\n",
        "embedding_model_split_train.fit(tmp_x, preproc_french_sentences_train, batch_size=1024, epochs=10, validation_split=0.2)\n",
        "\n",
        "# Print prediction(s)\n",
        "print(logits_to_text(embedding_model_split_train.predict(tmp_x[:1])[0], french_tokenizer_train))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "87/87 [==============================] - 21s 225ms/step - loss: 3.6726 - accuracy: 0.5162 - val_loss: 0.6291 - val_accuracy: 0.8093\n",
            "Epoch 2/10\n",
            "87/87 [==============================] - 19s 219ms/step - loss: 0.5304 - accuracy: 0.8335 - val_loss: 0.3901 - val_accuracy: 0.8688\n",
            "Epoch 3/10\n",
            "87/87 [==============================] - 19s 218ms/step - loss: 0.3628 - accuracy: 0.8782 - val_loss: 0.3266 - val_accuracy: 0.8905\n",
            "Epoch 4/10\n",
            "87/87 [==============================] - 19s 218ms/step - loss: 0.3004 - accuracy: 0.8980 - val_loss: 0.2920 - val_accuracy: 0.9003\n",
            "Epoch 5/10\n",
            "87/87 [==============================] - 19s 218ms/step - loss: 0.2648 - accuracy: 0.9084 - val_loss: 0.2758 - val_accuracy: 0.9048\n",
            "Epoch 6/10\n",
            "87/87 [==============================] - 19s 217ms/step - loss: 0.2517 - accuracy: 0.9119 - val_loss: 0.2462 - val_accuracy: 0.9147\n",
            "Epoch 7/10\n",
            "87/87 [==============================] - 19s 217ms/step - loss: 0.2281 - accuracy: 0.9194 - val_loss: 0.2394 - val_accuracy: 0.9174\n",
            "Epoch 8/10\n",
            "87/87 [==============================] - 19s 217ms/step - loss: 0.2221 - accuracy: 0.9214 - val_loss: 0.2377 - val_accuracy: 0.9180\n",
            "Epoch 9/10\n",
            "87/87 [==============================] - 19s 217ms/step - loss: 0.2156 - accuracy: 0.9233 - val_loss: 0.2347 - val_accuracy: 0.9194\n",
            "Epoch 10/10\n",
            "87/87 [==============================] - 19s 217ms/step - loss: 0.2129 - accuracy: 0.9238 - val_loss: 0.2358 - val_accuracy: 0.9189\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f095db5c7a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "la france est généralement froid à lautomne mais il est calme en février <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyTF66Uc0YyR"
      },
      "source": [
        "### Testing the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUehpGTV0awF"
      },
      "source": [
        "#preproc_english_sentences_test, preproc_french_sentences_test, english_tokenizer_test, french_tokenizer_test =\\\n",
        "    #preprocess(english_sentences_test, french_sentences_test)\n",
        "    \n",
        "#max_french_sequence_length_test = preproc_french_sentences_test.shape[1]\n",
        "\n",
        "#test_x = pad_sequences(sequences=preproc_english_sentences_test, maxlen=max_french_sequence_length_test, padding='post', value=0)\n",
        "#test_x = test_x.reshape((-1, preproc_french_sentences_test.shape[-2]))\n",
        "\n",
        "#embedding_model_score = embedding_model_split_train.evaluate(test_x, preproc_french_sentences_test, verbose=0)\n",
        "\n",
        "#print(\"Embedding model accuracy on unseen test data: {0:.2f}%\".format(embedding_model_score[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MGNCnZGrfuq"
      },
      "source": [
        "### Ids Back to Text BPE\n",
        "\n",
        "Here we use a similar logits_to_text function to convert the output integer sequences from the neural network to the French words. Since, we used different tokenization methods we had to change the function a bit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atAGWdrFvqY_"
      },
      "source": [
        "def logits_to_text_bpe(logits, index_to_words):\n",
        "    \"\"\"\n",
        "    Turn logits from a neural network into text using the tokenizer\n",
        "    :param logits: Logits from a neural network\n",
        "    :param tokenizer: Keras Tokenizer fit on the labels\n",
        "    :return: String that represents the text of the logits\n",
        "    \"\"\"\n",
        "    index_to_words = {v: k for k, v in index_to_words.items()}\n",
        "    index_to_words[0] = '<PAD>'\n",
        "    \n",
        "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKKPkbx44sBl"
      },
      "source": [
        "### RNN for BPE\n",
        "\n",
        "We use the same model for BPE as well. So, we can  check the accuracy and compare the results. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGp0icXOYUDn",
        "outputId": "3864d3b2-54b4-4b07-ea74-ce36c92454af"
      },
      "source": [
        "tmp_x = preproc_english_sentences_bpe.reshape((-1, preproc_french_sentences_bpe.shape[-2]))\n",
        "\n",
        "embedding_model_split_train = embed_model_split(tmp_x.shape,\n",
        "                                  max_french_sequence_length_train,\n",
        "                                  english_vocab_size_train + 1,\n",
        "                                  french_vocab_size_train + 1)\n",
        "\n",
        "embedding_model_split_train.fit(tmp_x, preproc_french_sentences_bpe, batch_size=1024, epochs=10, validation_split=0.2)\n",
        "\n",
        "# Print prediction(s)\n",
        "print(logits_to_text_bpe(embedding_model_split_train.predict(tmp_x[:1])[0], vocab_tokenization_fr))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "22/22 [==============================] - 28s 1s/step - loss: 2.7553 - accuracy: 0.6903 - val_loss: nan - val_accuracy: 0.9054\n",
            "Epoch 2/10\n",
            "22/22 [==============================] - 26s 1s/step - loss: 0.4150 - accuracy: 0.9095 - val_loss: nan - val_accuracy: 0.9229\n",
            "Epoch 3/10\n",
            "22/22 [==============================] - 26s 1s/step - loss: 0.3014 - accuracy: 0.9256 - val_loss: nan - val_accuracy: 0.9326\n",
            "Epoch 4/10\n",
            "22/22 [==============================] - 26s 1s/step - loss: 0.2491 - accuracy: 0.9340 - val_loss: nan - val_accuracy: 0.9379\n",
            "Epoch 5/10\n",
            "22/22 [==============================] - 26s 1s/step - loss: 0.2217 - accuracy: 0.9400 - val_loss: nan - val_accuracy: 0.9451\n",
            "Epoch 6/10\n",
            "22/22 [==============================] - 26s 1s/step - loss: 0.1934 - accuracy: 0.9467 - val_loss: nan - val_accuracy: 0.9492\n",
            "Epoch 7/10\n",
            "22/22 [==============================] - 26s 1s/step - loss: 0.1722 - accuracy: 0.9519 - val_loss: nan - val_accuracy: 0.9555\n",
            "Epoch 8/10\n",
            "22/22 [==============================] - 26s 1s/step - loss: 0.1531 - accuracy: 0.9564 - val_loss: nan - val_accuracy: 0.9591\n",
            "Epoch 9/10\n",
            "22/22 [==============================] - 26s 1s/step - loss: 0.1383 - accuracy: 0.9602 - val_loss: nan - val_accuracy: 0.9597\n",
            "Epoch 10/10\n",
            "22/22 [==============================] - 26s 1s/step - loss: 0.1265 - accuracy: 0.9632 - val_loss: nan - val_accuracy: 0.9660\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f095cf7bf80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "chine</w> chine</w> généralement</w> occupé</w> en</w> septembre</w> mais</w> il</w> est</w> parfois</w> froid</w> froid</w> printemps</w> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ydNrSd8YVnz"
      },
      "source": [
        "### RNN with Embedding\n",
        "\n",
        "Small changes to the model can increase the accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bF-BmOA4rUB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07c39e71-7242-4b34-8550-b0024f74f7a5"
      },
      "source": [
        "def embed_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "    \"\"\"\n",
        "    Build and train a RNN model using word embedding on x and y\n",
        "    :param input_shape: Tuple of input shape\n",
        "    :param output_sequence_length: Length of output sequence\n",
        "    :param english_vocab_size: Number of unique English words in the dataset\n",
        "    :param french_vocab_size: Number of unique French words in the dataset\n",
        "    :return: Keras model built, but not trained\n",
        "    \"\"\"\n",
        "    # TODO: Implement\n",
        "\n",
        "    # Hyperparameters\n",
        "    learning_rate = 0.005\n",
        "    \n",
        "    # TODO: Build the layers\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(english_vocab_size, 256, input_length=input_shape[1], input_shape=input_shape[1:]))\n",
        "    model.add(GRU(256, return_sequences=True))    \n",
        "    model.add(TimeDistributed(Dense(1024, activation='relu')))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax'))) \n",
        "\n",
        "    # Compile model\n",
        "    model.compile(loss=sparse_categorical_crossentropy,\n",
        "                  optimizer=Adam(learning_rate),\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "#test_embed_model(embed_model)\n",
        "\n",
        "# TODO: Reshape the input\n",
        "tmp_x = preproc_english_sentences_bpe.reshape((-1, preproc_french_sentences_bpe.shape[-2]))\n",
        "\n",
        "# TODO: Train the neural network\n",
        "embed_rnn_model = embed_model(\n",
        "    tmp_x.shape,\n",
        "    preproc_french_sentences_bpe.shape[1],\n",
        "    english_vocab_size_bpe+1,\n",
        "    french_vocab_size_bpe+1)\n",
        "\n",
        "embed_rnn_model.summary()\n",
        "\n",
        "embed_rnn_model.fit(tmp_x, preproc_french_sentences_bpe, batch_size=1024, epochs=10, validation_split=0.2)\n",
        "\n",
        "# TODO: Print prediction(s)\n",
        "print(logits_to_text_bpe(embed_rnn_model.predict(tmp_x[:1])[0], vocab_tokenization_fr))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_19 (Embedding)     (None, 115, 256)          51200     \n",
            "_________________________________________________________________\n",
            "gru_19 (GRU)                 (None, 115, 256)          394752    \n",
            "_________________________________________________________________\n",
            "time_distributed_20 (TimeDis (None, 115, 1024)         263168    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 115, 1024)         0         \n",
            "_________________________________________________________________\n",
            "time_distributed_21 (TimeDis (None, 115, 345)          353625    \n",
            "=================================================================\n",
            "Total params: 1,062,745\n",
            "Trainable params: 1,062,745\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "87/87 [==============================] - 62s 688ms/step - loss: 1.0652 - accuracy: 0.8524 - val_loss: 0.2294 - val_accuracy: 0.9397\n",
            "Epoch 2/10\n",
            "87/87 [==============================] - 59s 681ms/step - loss: 0.2040 - accuracy: 0.9455 - val_loss: 0.1225 - val_accuracy: 0.9645\n",
            "Epoch 3/10\n",
            "87/87 [==============================] - 59s 681ms/step - loss: 0.1208 - accuracy: 0.9646 - val_loss: 0.0866 - val_accuracy: 0.9729\n",
            "Epoch 4/10\n",
            "87/87 [==============================] - 59s 681ms/step - loss: 0.0903 - accuracy: 0.9723 - val_loss: 0.0729 - val_accuracy: 0.9765\n",
            "Epoch 5/10\n",
            "87/87 [==============================] - 59s 681ms/step - loss: 0.0763 - accuracy: 0.9759 - val_loss: 0.0628 - val_accuracy: 0.9791\n",
            "Epoch 6/10\n",
            "87/87 [==============================] - 59s 681ms/step - loss: 0.0663 - accuracy: 0.9787 - val_loss: 0.0563 - val_accuracy: 0.9811\n",
            "Epoch 7/10\n",
            "87/87 [==============================] - 59s 681ms/step - loss: 0.0597 - accuracy: 0.9805 - val_loss: 0.0534 - val_accuracy: 0.9821\n",
            "Epoch 8/10\n",
            "87/87 [==============================] - 59s 682ms/step - loss: 0.0554 - accuracy: 0.9818 - val_loss: 0.0501 - val_accuracy: 0.9832\n",
            "Epoch 9/10\n",
            "87/87 [==============================] - 59s 680ms/step - loss: 0.0529 - accuracy: 0.9824 - val_loss: 0.0472 - val_accuracy: 0.9838\n",
            "Epoch 10/10\n",
            "87/87 [==============================] - 59s 682ms/step - loss: 0.0493 - accuracy: 0.9835 - val_loss: 0.0466 - val_accuracy: 0.9841\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f09a159e830> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "la</w> france</w> est</w> généralement</w> froid</w> à</w> lautomne</w> mais</w> il</w> est</w> calme</w> en</w> février</w> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7CO6SRptF74"
      },
      "source": [
        "### Test the model on the test data set\n",
        "\n",
        "See how the model trained on the training set data performs on the unseen test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J82ut0wC4-eE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00c9020e-d5a4-4d62-8804-7a2ce317aa73"
      },
      "source": [
        "preproc_english_sentences_bpe, preproc_french_sentences_bpe, english_tokenizer_bpe, french_tokenizer_bpe=preprocess_BPE(english_sentences_test, french_sentences_test)\n",
        "\n",
        "test_x = preproc_english_sentences_bpe.reshape((-1, preproc_french_sentences_bpe.shape[-2]))\n",
        "\n",
        "embedding_model_score = embed_rnn_model.evaluate(test_x, preproc_french_sentences_bpe, verbose=0)\n",
        "\n",
        "print(\"Embedding model accuracy on unseen test data: {0:.2f}%\".format(embedding_model_score[1]*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Embedding model accuracy on unseen test data: 98.43%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}